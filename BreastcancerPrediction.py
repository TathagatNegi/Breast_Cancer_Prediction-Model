# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kmcj8SxTnw95N9hjhwiCQRS0BOxJaHOq
"""

#DESCRIPTION: THIS PROGRAM DETECTS BREAST CANCER, BASED OF DATA.

#import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#load the data
from google.colab import files
uploaded = files.upload()
df= pd.read_csv('data.csv')
df.head(7)

#count the no of rows and col
df.shape
#569 patient

#count of no of empty values in each col
df.isna().sum()

#drop the col 32
df =df.dropna(axis=1)

#get new count
df.shape

#count of no of malignant (M) or benign (B) cells
df['diagnosis'].value_counts()

#visualize
sns.countplot(df['diagnosis'], label ='count')

#look at the data types to see which col needs to be encoded
df.dtypes

#encode the categorical data value
from sklearn.preprocessing import LabelEncoder
labelencoder_Y = LabelEncoder()
df.iloc[:,1] =labelencoder_Y.fit_transform(df.iloc[:,1].values)

#create a pair plot
sns.pairplot(df.iloc[:,1:6], hue='diagnosis')

#print first 5 rows
df.head(5)

#get the correlation of the col
df.iloc[:,1:12].corr()

#visualize the correlation
plt.figure(figsize=(10,10))
sns.heatmap(df.iloc[:,1:12].corr(), fmt = ',0%')

#split the dataset into independent (X) and dependent (Y) sets
X = df.iloc[:,2:31].values
Y = df.iloc[:,1].values

#split into 75% training and 25% testing
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.25, random_state = 0)

#scale the data feature scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train =sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

X_train

# func for models
def models(X_train, Y_train):

  #logistic regression
  from sklearn.linear_model import LogisticRegression
  log = LogisticRegression(random_state=0)
  log.fit(X_train, Y_train)

  #Decision tree
  from sklearn.tree import DecisionTreeClassifier
  tree = DecisionTreeClassifier(criterion = 'entropy', random_state=0)
  tree.fit(X_train,Y_train)

  #random forest classifer
  from sklearn.ensemble import RandomForestClassifier
  forest = RandomForestClassifier(n_estimators =10, criterion ='entropy', random_state=0)
  forest.fit(X_train,Y_train)

  #print model accuracy
  print('[0]logistic Regression training Accuracy:', log.score(X_train,Y_train))
  print('[1]Decison Tree Classifier training Accuracy:', tree.score(X_train,Y_train))
  print('[2]Forest Classifier training Accuracy:', forest.score(X_train,Y_train))

  return log,tree,forest

model = models(X_train,Y_train)

#test model accuracy on test data on confusion matrix
from sklearn.metrics import confusion_matrix

for i in range( len(model)):
  print('Model', i)
  cm = confusion_matrix(Y_test, model[0].predict(X_test))
  TP = cm[0][0]
  TN = cm[1][1]
  FP = cm[1][0]
  FN = cm[0][1]
  print(cm)
  print('testing Accuracy =', (TP + TN)/(TP+TN+FN+FP))
  print()

#show matrics of model

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

for i in range( len(model)):
  print('Model', i)
  print( classification_report(Y_test, model[i].predict(X_test)))
  print(  accuracy_score(Y_test, model[i].predict(X_test)))
  print()

#print the prediction of RFC model
pred =model[2].predict(X_test)
print(pred)
print()
print(Y_test)